{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Abdulkadir\\\\anaconda3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# have my file path set to python not anaconda\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[*]\") \\\n",
    ".appName('DE-ZOOM').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark jobs can be seen in port 4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the high volume data set \n",
    "!curl -sS https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet > data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wc -l data.parquet # works on linux (gitbash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data.parquet', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B03404|              B03404|2022-01-01 00:05:31|2022-01-01 00:05:40|2022-01-01 00:07:24|2022-01-01 00:18:28|         170|         161|      1.18|      664|               24.9|  0.0|0.75|     2.21|                2.75|        0.0| 0.0|     23.03|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2022-01-01 00:19:27|2022-01-01 00:22:08|2022-01-01 00:22:32|2022-01-01 00:30:12|         237|         161|      0.82|      460|              11.97|  0.0|0.36|     1.06|                2.75|        0.0| 0.0|     12.32|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2022-01-01 00:43:53|2022-01-01 00:57:37|2022-01-01 00:57:37|2022-01-01 01:07:32|         237|         161|      1.18|      595|              29.82|  0.0|0.89|     2.65|                2.75|        0.0| 0.0|      23.3|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2022-01-01 00:15:36|2022-01-01 00:17:08|2022-01-01 00:18:02|2022-01-01 00:23:05|         262|         229|      1.65|      303|               7.91|  0.0|0.24|      0.7|                2.75|        0.0| 0.0|       6.3|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2022-01-01 00:25:45|2022-01-01 00:26:01|2022-01-01 00:28:01|2022-01-01 00:35:42|         229|         141|      1.65|      461|               9.44|  0.0|0.28|     0.84|                2.75|        0.0| 0.0|      7.44|                  N|                N|                  |               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually changing schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "originating_base_num            object\n",
       "request_datetime        datetime64[ns]\n",
       "on_scene_datetime       datetime64[ns]\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "trip_miles                     float64\n",
       "trip_time                        int64\n",
       "base_passenger_fare            float64\n",
       "tolls                          float64\n",
       "bcf                            float64\n",
       "sales_tax                      float64\n",
       "congestion_surcharge           float64\n",
       "airport_fee                    float64\n",
       "tips                           float64\n",
       "driver_pay                     float64\n",
       "shared_request_flag             object\n",
       "shared_match_flag               object\n",
       "access_a_ride_flag              object\n",
       "wav_request_flag                object\n",
       "wav_match_flag                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).toPandas().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampType(), True), StructField('on_scene_datetime', TimestampType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df.limit(10).toPandas()).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, StructField, TimestampType, LongType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "        types.StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True),\n",
    "        types.StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampType(), True),\n",
    "        types.StructField('on_scene_datetime', TimestampType(), True), StructField('pickup_datetime', TimestampType(), True),\n",
    "        types.StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', LongType(), True),\n",
    "        types.StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True),\n",
    "        types.StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True),\n",
    "        types.StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True),\n",
    "        types.StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True),\n",
    "        types.StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True),\n",
    "        types.StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True),\n",
    "        types.StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True),\n",
    "        types.StructField('wav_request_flag', StringType(), True),\n",
    "        types.StructField('wav_match_flag', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a large fiel and split it into multiple partitions\n",
    "df_repartitioned = df.repartition(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, originating_base_num: string, request_datetime: timestamp, on_scene_datetime: timestamp, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: bigint, DOLocationID: bigint, trip_miles: double, trip_time: bigint, base_passenger_fare: double, tolls: double, bcf: double, sales_tax: double, congestion_surcharge: double, airport_fee: double, tips: double, driver_pay: double, shared_request_flag: string, shared_match_flag: string, access_a_ride_flag: string, wav_request_flag: string, wav_match_flag: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repartitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creates a new folder\n",
    "df_repartitioned.write.parquet('output_partitioned/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading partitioned data\n",
    "df = spark.read.parquet('output_partitioned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14751591, 24)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of df\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'originating_base_num',\n",
       " 'request_datetime',\n",
       " 'on_scene_datetime',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_miles',\n",
       " 'trip_time',\n",
       " 'base_passenger_fare',\n",
       " 'tolls',\n",
       " 'bcf',\n",
       " 'sales_tax',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'tips',\n",
       " 'driver_pay',\n",
       " 'shared_request_flag',\n",
       " 'shared_match_flag',\n",
       " 'access_a_ride_flag',\n",
       " 'wav_request_flag',\n",
       " 'wav_match_flag']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|trip_miles|trip_time|\n",
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "|           HV0003|2022-01-02 14:29:56|2022-01-02 14:50:35|     12.35|     1239|\n",
      "|           HV0003|2022-01-17 20:44:30|2022-01-17 20:53:03|      1.47|      514|\n",
      "|           HV0003|2022-01-26 09:16:13|2022-01-26 09:23:22|      1.08|      429|\n",
      "|           HV0005|2022-01-19 13:18:39|2022-01-19 13:45:24|     3.969|     1605|\n",
      "|           HV0005|2022-01-17 13:20:19|2022-01-17 13:49:10|     8.987|     1731|\n",
      "|           HV0003|2022-01-24 21:40:23|2022-01-24 21:53:22|      1.87|      779|\n",
      "|           HV0003|2022-01-11 09:02:59|2022-01-11 09:19:16|       2.7|      977|\n",
      "|           HV0005|2022-01-11 11:46:30|2022-01-11 12:04:54|     8.068|     1104|\n",
      "|           HV0003|2022-01-10 08:25:14|2022-01-10 08:35:34|      1.28|      620|\n",
      "|           HV0003|2022-01-08 22:51:31|2022-01-08 23:08:48|      2.81|     1037|\n",
      "|           HV0005|2022-01-21 22:59:58|2022-01-21 23:05:24|     0.826|      326|\n",
      "|           HV0003|2022-01-22 07:17:21|2022-01-22 10:06:21|     65.58|    10140|\n",
      "|           HV0005|2022-01-24 15:51:09|2022-01-24 15:55:47|     1.173|      278|\n",
      "|           HV0003|2022-01-23 09:38:40|2022-01-23 10:01:28|     13.02|     1368|\n",
      "|           HV0005|2022-01-23 15:19:20|2022-01-23 15:29:54|       0.9|      634|\n",
      "|           HV0003|2022-01-24 07:45:44|2022-01-24 08:08:23|      3.42|     1359|\n",
      "|           HV0005|2022-01-21 09:51:08|2022-01-21 10:03:51|     7.299|      763|\n",
      "|           HV0003|2022-01-01 23:32:19|2022-01-01 23:35:43|      0.82|      204|\n",
      "|           HV0003|2022-01-27 06:07:44|2022-01-27 06:20:42|      2.62|      778|\n",
      "|           HV0005|2022-01-07 19:19:49|2022-01-07 19:39:01|     3.398|     1152|\n",
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|trip_miles|trip_time|\n",
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "|           HV0003|2022-01-02 14:29:56|2022-01-02 14:50:35|     12.35|     1239|\n",
      "|           HV0003|2022-01-17 20:44:30|2022-01-17 20:53:03|      1.47|      514|\n",
      "|           HV0003|2022-01-26 09:16:13|2022-01-26 09:23:22|      1.08|      429|\n",
      "|           HV0005|2022-01-19 13:18:39|2022-01-19 13:45:24|     3.969|     1605|\n",
      "|           HV0005|2022-01-17 13:20:19|2022-01-17 13:49:10|     8.987|     1731|\n",
      "|           HV0003|2022-01-24 21:40:23|2022-01-24 21:53:22|      1.87|      779|\n",
      "|           HV0003|2022-01-11 09:02:59|2022-01-11 09:19:16|       2.7|      977|\n",
      "|           HV0005|2022-01-11 11:46:30|2022-01-11 12:04:54|     8.068|     1104|\n",
      "|           HV0003|2022-01-10 08:25:14|2022-01-10 08:35:34|      1.28|      620|\n",
      "|           HV0003|2022-01-08 22:51:31|2022-01-08 23:08:48|      2.81|     1037|\n",
      "|           HV0005|2022-01-21 22:59:58|2022-01-21 23:05:24|     0.826|      326|\n",
      "|           HV0003|2022-01-22 07:17:21|2022-01-22 10:06:21|     65.58|    10140|\n",
      "|           HV0005|2022-01-24 15:51:09|2022-01-24 15:55:47|     1.173|      278|\n",
      "|           HV0003|2022-01-23 09:38:40|2022-01-23 10:01:28|     13.02|     1368|\n",
      "|           HV0005|2022-01-23 15:19:20|2022-01-23 15:29:54|       0.9|      634|\n",
      "|           HV0003|2022-01-24 07:45:44|2022-01-24 08:08:23|      3.42|     1359|\n",
      "|           HV0005|2022-01-21 09:51:08|2022-01-21 10:03:51|     7.299|      763|\n",
      "|           HV0003|2022-01-01 23:32:19|2022-01-01 23:35:43|      0.82|      204|\n",
      "|           HV0003|2022-01-27 06:07:44|2022-01-27 06:20:42|      2.62|      778|\n",
      "|           HV0005|2022-01-07 19:19:49|2022-01-07 19:39:01|     3.398|     1152|\n",
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|trip_miles|trip_time|\n",
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "|           HV0003|2022-01-02 14:29:56|2022-01-02 14:50:35|     12.35|     1239|\n",
      "|           HV0003|2022-01-22 07:17:21|2022-01-22 10:06:21|     65.58|    10140|\n",
      "|           HV0003|2022-01-23 09:38:40|2022-01-23 10:01:28|     13.02|     1368|\n",
      "|           HV0005|2022-01-14 11:14:30|2022-01-14 11:52:54|    14.021|     2304|\n",
      "|           HV0005|2022-01-15 19:22:02|2022-01-15 19:52:02|    11.574|     1800|\n",
      "|           HV0003|2022-01-21 08:02:12|2022-01-21 08:31:15|     10.18|     1743|\n",
      "|           HV0003|2022-01-13 17:53:22|2022-01-13 18:49:14|     15.65|     3352|\n",
      "|           HV0003|2022-01-27 06:25:41|2022-01-27 06:58:25|     12.49|     1964|\n",
      "|           HV0003|2022-01-05 10:45:08|2022-01-05 11:11:52|     10.77|     1605|\n",
      "|           HV0003|2022-01-07 10:42:40|2022-01-07 11:32:03|     36.22|     2963|\n",
      "|           HV0003|2022-01-06 10:03:06|2022-01-06 10:38:50|     16.99|     2144|\n",
      "|           HV0005|2022-01-20 23:19:33|2022-01-20 23:39:45|    10.254|     1212|\n",
      "|           HV0005|2022-01-11 09:31:05|2022-01-11 09:53:42|    17.607|     1403|\n",
      "|           HV0003|2022-01-02 01:05:05|2022-01-02 01:45:03|     16.51|     2399|\n",
      "|           HV0003|2022-01-17 18:27:39|2022-01-17 19:03:17|      16.1|     2138|\n",
      "|           HV0005|2022-01-23 17:33:02|2022-01-23 18:03:41|    11.655|     1839|\n",
      "|           HV0005|2022-01-31 20:47:29|2022-01-31 21:31:00|    16.922|     2611|\n",
      "|           HV0003|2022-01-20 05:20:07|2022-01-20 05:41:09|     12.43|     1262|\n",
      "|           HV0003|2022-01-07 17:01:28|2022-01-07 17:36:45|     10.26|     2117|\n",
      "|           HV0003|2022-01-31 21:14:36|2022-01-31 21:43:05|     10.93|     1709|\n",
      "+-----------------+-------------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where statement\n",
    "df[df['trip_miles'] > 10]['hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why pyspark? Spark has user defined functions. There is a list of predefined functions, but you can also use your own functions like .apply() or .map() in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+----------+---------+----------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|trip_miles|trip_time|pickup_day|\n",
      "+-----------------+-------------------+-------------------+----------+---------+----------+\n",
      "|           HV0003|2022-01-02 14:29:56|2022-01-02 14:50:35|     12.35|     1239|         1|\n",
      "|           HV0003|2022-01-17 20:44:30|2022-01-17 20:53:03|      1.47|      514|         2|\n",
      "|           HV0003|2022-01-26 09:16:13|2022-01-26 09:23:22|      1.08|      429|         4|\n",
      "|           HV0005|2022-01-19 13:18:39|2022-01-19 13:45:24|     3.969|     1605|         4|\n",
      "|           HV0005|2022-01-17 13:20:19|2022-01-17 13:49:10|     8.987|     1731|         2|\n",
      "|           HV0003|2022-01-24 21:40:23|2022-01-24 21:53:22|      1.87|      779|         2|\n",
      "|           HV0003|2022-01-11 09:02:59|2022-01-11 09:19:16|       2.7|      977|         3|\n",
      "|           HV0005|2022-01-11 11:46:30|2022-01-11 12:04:54|     8.068|     1104|         3|\n",
      "|           HV0003|2022-01-10 08:25:14|2022-01-10 08:35:34|      1.28|      620|         2|\n",
      "|           HV0003|2022-01-08 22:51:31|2022-01-08 23:08:48|      2.81|     1037|         7|\n",
      "|           HV0005|2022-01-21 22:59:58|2022-01-21 23:05:24|     0.826|      326|         6|\n",
      "|           HV0003|2022-01-22 07:17:21|2022-01-22 10:06:21|     65.58|    10140|         7|\n",
      "|           HV0005|2022-01-24 15:51:09|2022-01-24 15:55:47|     1.173|      278|         2|\n",
      "|           HV0003|2022-01-23 09:38:40|2022-01-23 10:01:28|     13.02|     1368|         1|\n",
      "|           HV0005|2022-01-23 15:19:20|2022-01-23 15:29:54|       0.9|      634|         1|\n",
      "|           HV0003|2022-01-24 07:45:44|2022-01-24 08:08:23|      3.42|     1359|         2|\n",
      "|           HV0005|2022-01-21 09:51:08|2022-01-21 10:03:51|     7.299|      763|         6|\n",
      "|           HV0003|2022-01-01 23:32:19|2022-01-01 23:35:43|      0.82|      204|         7|\n",
      "|           HV0003|2022-01-27 06:07:44|2022-01-27 06:20:42|      2.62|      778|         5|\n",
      "|           HV0005|2022-01-07 19:19:49|2022-01-07 19:39:01|     3.398|     1152|         6|\n",
      "+-----------------+-------------------+-------------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time'] \\\n",
    ".withColumn('pickup_day', F.dayofweek('pickup_datetime')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divisible(base_num):\n",
    "    num = int(base_num[2:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a/003'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisible('HV0003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_udf = F.udf(divisible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+----------+---------+-------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|trip_miles|trip_time|base_id|\n",
      "+-----------------+-------------------+-------------------+----------+---------+-------+\n",
      "|           HV0003|2022-01-02 14:29:56|2022-01-02 14:50:35|     12.35|     1239|  a/003|\n",
      "|           HV0003|2022-01-17 20:44:30|2022-01-17 20:53:03|      1.47|      514|  a/003|\n",
      "|           HV0003|2022-01-26 09:16:13|2022-01-26 09:23:22|      1.08|      429|  a/003|\n",
      "|           HV0005|2022-01-19 13:18:39|2022-01-19 13:45:24|     3.969|     1605|  e/005|\n",
      "|           HV0005|2022-01-17 13:20:19|2022-01-17 13:49:10|     8.987|     1731|  e/005|\n",
      "|           HV0003|2022-01-24 21:40:23|2022-01-24 21:53:22|      1.87|      779|  a/003|\n",
      "|           HV0003|2022-01-11 09:02:59|2022-01-11 09:19:16|       2.7|      977|  a/003|\n",
      "|           HV0005|2022-01-11 11:46:30|2022-01-11 12:04:54|     8.068|     1104|  e/005|\n",
      "|           HV0003|2022-01-10 08:25:14|2022-01-10 08:35:34|      1.28|      620|  a/003|\n",
      "|           HV0003|2022-01-08 22:51:31|2022-01-08 23:08:48|      2.81|     1037|  a/003|\n",
      "|           HV0005|2022-01-21 22:59:58|2022-01-21 23:05:24|     0.826|      326|  e/005|\n",
      "|           HV0003|2022-01-22 07:17:21|2022-01-22 10:06:21|     65.58|    10140|  a/003|\n",
      "|           HV0005|2022-01-24 15:51:09|2022-01-24 15:55:47|     1.173|      278|  e/005|\n",
      "|           HV0003|2022-01-23 09:38:40|2022-01-23 10:01:28|     13.02|     1368|  a/003|\n",
      "|           HV0005|2022-01-23 15:19:20|2022-01-23 15:29:54|       0.9|      634|  e/005|\n",
      "|           HV0003|2022-01-24 07:45:44|2022-01-24 08:08:23|      3.42|     1359|  a/003|\n",
      "|           HV0005|2022-01-21 09:51:08|2022-01-21 10:03:51|     7.299|      763|  e/005|\n",
      "|           HV0003|2022-01-01 23:32:19|2022-01-01 23:35:43|      0.82|      204|  a/003|\n",
      "|           HV0003|2022-01-27 06:07:44|2022-01-27 06:20:42|      2.62|      778|  a/003|\n",
      "|           HV0005|2022-01-07 19:19:49|2022-01-07 19:39:01|     3.398|     1152|  e/005|\n",
      "+-----------------+-------------------+-------------------+----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['hvfhs_license_num', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'trip_time'] \\\n",
    ".withColumn('base_id', my_udf(df['hvfhs_license_num'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow and green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stars indicate all files in that folder\n",
    "df_yellow = spark.read.parquet('data/raw/yellow/*/*', header=True, inferSchema=True)\n",
    "df_green = spark.read.parquet('data/raw/green/*/*', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow.printSchema(), df_yellow.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n"
     ]
    }
   ],
   "source": [
    "print(df_green.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n"
     ]
    }
   ],
   "source": [
    "print(df_yellow.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only take matching columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOLocationID',\n",
       " 'PULocationID',\n",
       " 'RatecodeID',\n",
       " 'VendorID',\n",
       " 'congestion_surcharge',\n",
       " 'extra',\n",
       " 'fare_amount',\n",
       " 'improvement_surcharge',\n",
       " 'mta_tax',\n",
       " 'passenger_count',\n",
       " 'payment_type',\n",
       " 'store_and_fwd_flag',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount',\n",
       " 'trip_distance'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_green.columns) & set(df_yellow.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also include pick up and drop off times as they have different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOLocationID',\n",
       " 'PULocationID',\n",
       " 'RatecodeID',\n",
       " 'VendorID',\n",
       " 'congestion_surcharge',\n",
       " 'dropoff_datetime',\n",
       " 'extra',\n",
       " 'fare_amount',\n",
       " 'improvement_surcharge',\n",
       " 'mta_tax',\n",
       " 'passenger_count',\n",
       " 'payment_type',\n",
       " 'pickup_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount',\n",
       " 'trip_distance'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_green.columns) & set(df_yellow.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preserve order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = []\n",
    "for col in df_green.columns:\n",
    "    if col in df_yellow.columns:\n",
    "        common_columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When joining the data together, need to identify whether green or yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green[common_columns]\n",
    "df_green = df_green.withColumn('service_type', F.lit('green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow[common_columns]\n",
    "df_yellow = df_yellow.withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data = df_green.unionAll(df_yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 2802931|\n",
      "|      yellow|55553400|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data.createOrReplaceTempView('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|service_type|count(service_type)|\n",
      "+------------+-------------------+\n",
      "|       green|            2802931|\n",
      "|      yellow|           55553400|\n",
      "+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        service_type,\n",
    "        COUNT(service_type)\n",
    "    FROM \n",
    "        trips_data\n",
    "    GROUP BY\n",
    "        service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce writes it to one file\n",
    "df_result.coalesce(1) \\\n",
    "    .write.parquet('data/report/revenue/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.createOrReplaceTempView('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    date_trunc('hour', pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM \n",
    "    green\n",
    "WHERE \n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "ORDER BY \n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2020-01-01 00:00:00|   7| 769.7299999999997|            45|\n",
      "|2020-01-01 00:00:00|  17|            195.03|             9|\n",
      "|2020-01-01 00:00:00|  18|               7.8|             1|\n",
      "|2020-01-01 00:00:00|  22|              15.8|             1|\n",
      "|2020-01-01 00:00:00|  24|              87.6|             3|\n",
      "|2020-01-01 00:00:00|  25|             531.0|            26|\n",
      "|2020-01-01 00:00:00|  29|              61.3|             1|\n",
      "|2020-01-01 00:00:00|  32| 68.94999999999999|             2|\n",
      "|2020-01-01 00:00:00|  33|317.27000000000004|            11|\n",
      "|2020-01-01 00:00:00|  35|            129.96|             5|\n",
      "|2020-01-01 00:00:00|  36|295.34000000000003|            11|\n",
      "|2020-01-01 00:00:00|  37|            175.67|             6|\n",
      "|2020-01-01 00:00:00|  38| 98.78999999999999|             2|\n",
      "|2020-01-01 00:00:00|  40|            168.98|             8|\n",
      "|2020-01-01 00:00:00|  41| 1363.959999999999|            84|\n",
      "|2020-01-01 00:00:00|  42| 799.7600000000002|            52|\n",
      "|2020-01-01 00:00:00|  43|            107.52|             6|\n",
      "|2020-01-01 00:00:00|  47|              13.3|             1|\n",
      "|2020-01-01 00:00:00|  49|            266.76|            14|\n",
      "|2020-01-01 00:00:00|  51|              17.8|             2|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition writes the data to 20 different files\n",
    "df_green_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow.createOrReplaceTempView('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    date_trunc('hour', pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM \n",
    "    yellow\n",
    "WHERE \n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "ORDER BY \n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------+\n",
      "|               hour|zone|            amount|number_records|\n",
      "+-------------------+----+------------------+--------------+\n",
      "|2020-01-01 00:00:00|   3|              25.0|             1|\n",
      "|2020-01-01 00:00:00|   4|1004.2999999999995|            57|\n",
      "|2020-01-01 00:00:00|   7| 455.1700000000002|            38|\n",
      "|2020-01-01 00:00:00|  10|             42.41|             2|\n",
      "|2020-01-01 00:00:00|  12|             107.0|             6|\n",
      "|2020-01-01 00:00:00|  13|1214.7999999999993|            56|\n",
      "|2020-01-01 00:00:00|  14|               8.8|             1|\n",
      "|2020-01-01 00:00:00|  15|             34.09|             1|\n",
      "|2020-01-01 00:00:00|  17|220.20999999999998|             8|\n",
      "|2020-01-01 00:00:00|  18|               5.8|             1|\n",
      "|2020-01-01 00:00:00|  24| 754.9499999999997|            45|\n",
      "|2020-01-01 00:00:00|  25| 324.3500000000001|            16|\n",
      "|2020-01-01 00:00:00|  32|              18.0|             1|\n",
      "|2020-01-01 00:00:00|  33|            255.56|             8|\n",
      "|2020-01-01 00:00:00|  34|              19.3|             1|\n",
      "|2020-01-01 00:00:00|  36|            109.17|             3|\n",
      "|2020-01-01 00:00:00|  37|            161.61|             7|\n",
      "|2020-01-01 00:00:00|  40|             89.97|             5|\n",
      "|2020-01-01 00:00:00|  41|1256.5299999999988|            80|\n",
      "|2020-01-01 00:00:00|  42| 635.3500000000001|            46|\n",
      "+-------------------+----+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue \\\n",
    "    .repartition(20) \\\n",
    "    .write.parquet('data/report/revenue/yellow', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_revenue = df_green_revenue \\\n",
    "    .withColumnRenamed('amount', 'green_amount') \\\n",
    "    .withColumnRenamed('number_records', 'green_number_records') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = df_yellow_revenue \\\n",
    "    .withColumnRenamed('amount', 'yellow_amount') \\\n",
    "    .withColumnRenamed('number_records', 'yellow_number_records') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_revenue.join(df_yellow_revenue, on=['hour', 'zone'], how='outer') # left outer --> everything in green to match in yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "|               hour|zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|\n",
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "|2020-01-01 00:00:00|  36|295.34000000000003|                  11|            109.17|                    3|\n",
      "|2020-01-01 00:00:00|  68|              null|                null| 7825.070000000024|                  396|\n",
      "|2020-01-01 00:00:00|  70|54.900000000000006|                   3|               9.3|                    1|\n",
      "|2020-01-01 00:00:00|  73|              null|                null|              17.3|                    1|\n",
      "|2020-01-01 00:00:00|  76|            143.78|                   4|             35.51|                    2|\n",
      "|2020-01-01 00:00:00| 136|            111.68|                   2|             168.0|                    4|\n",
      "|2020-01-01 00:00:00| 151|              null|                null| 2536.959999999999|                  148|\n",
      "|2020-01-01 00:00:00| 152|58.519999999999996|                   6|251.73000000000008|                   16|\n",
      "|2020-01-01 00:00:00| 223|            149.48|                   8|             13.57|                    1|\n",
      "|2020-01-01 00:00:00| 242|             64.25|                   1|              null|                 null|\n",
      "|2020-01-01 00:00:00| 247|            117.66|                   5|               9.3|                    1|\n",
      "|2020-01-01 00:00:00| 254| 73.82000000000001|                   3|            146.04|                    4|\n",
      "|2020-01-01 01:00:00|   7| 690.2099999999999|                  42|1004.7099999999994|                   62|\n",
      "|2020-01-01 01:00:00|  24|126.41999999999999|                   6| 783.8999999999999|                   44|\n",
      "|2020-01-01 01:00:00|  41| 819.4999999999998|                  61|1485.2899999999981|                   98|\n",
      "|2020-01-01 01:00:00| 100|              null|                null| 1711.839999999999|                   72|\n",
      "|2020-01-01 01:00:00| 101|              30.3|                   1|              null|                 null|\n",
      "|2020-01-01 01:00:00| 138|              null|                null|241.95000000000005|                    7|\n",
      "|2020-01-01 01:00:00| 148|              null|                null| 8202.780000000024|                  412|\n",
      "|2020-01-01 01:00:00| 163|              null|                null| 5483.990000000014|                  286|\n",
      "+-------------------+----+------------------+--------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.write.parquet('data/report/total', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge',\n",
       " 'service_type']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSELECT\\n    date_trunc('hour', pickup_datetime) AS hour,\\n    PULocationID AS zone,\\n    SUM(total_amount) AS amount,\\n    COUNT(1) AS number_records\\nFROM \\n    green\\nWHERE \\n    pickup_datetime >= '2020-01-01 00:00:00'\\nGROUP BY\\n    1, 2\\nORDER BY \\n    1, 2\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement this using rdd\n",
    "\"\"\"\n",
    "SELECT\n",
    "    date_trunc('hour', pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM \n",
    "    green\n",
    "WHERE \n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "ORDER BY \n",
    "    1, 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT\n",
    "rdd_result = df_green['pickup_datetime', 'PULocationID', 'total_amount'].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2019, 12, 18, 15, 52, 30), PULocationID=264, total_amount=4.81),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), PULocationID=66, total_amount=24.36),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), PULocationID=181, total_amount=15.34),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), PULocationID=129, total_amount=25.05),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), PULocationID=210, total_amount=11.3)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_result.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE\n",
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime(2020, 1, 1)\n",
    "\n",
    "rdd_result = rdd_result.filter(lambda row: row['pickup_datetime'] >= start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), PULocationID=66, total_amount=24.36),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), PULocationID=181, total_amount=15.34),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), PULocationID=129, total_amount=25.05),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), PULocationID=210, total_amount=11.3),\n",
       " Row(pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 33), PULocationID=35, total_amount=14.8)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_result.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    hour = row['pickup_datetime'].replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row['PULocationID']\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row['total_amount']\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "    \n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "rdd_result = rdd_result.map(prepare_for_grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = rdd_result.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result.repartition(50) \\\n",
    "    .write.parquet('data/report/rdd/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = spark.read.parquet('data/report/rdd/', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = rdd_result.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce by key\n",
    "def calculate_revenue(left, right):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = rdd_result.reduceByKey(calculate_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1], \n",
    "        revenue=row[1][0], \n",
    "        count=row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[200] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_result.map(unwrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_result.toDF().show()\n",
    "# job cannot handle capacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "47522bcd6fb3c7f1153a388e6a10376d870d5641f7ee2cacec881af737952fcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
